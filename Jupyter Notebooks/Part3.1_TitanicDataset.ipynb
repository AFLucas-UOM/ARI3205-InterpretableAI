{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project ARI3205 Interpretable AI for Deep Learning Models *(Part 3)*\n",
    "---\n",
    "\n",
    "**Name:** Sean David Muscat \n",
    "\n",
    "**ID No:** 0172004L\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# First install our libraries for Part 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install alibi[tensorflow] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[92m✔\u001b[0m] Library 'tensorflow' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'scikit-learn' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'matplotlib' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'seaborn' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'pandas' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'numpy' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'scipy' is already installed.\n",
      "[\u001b[92m✔\u001b[0m] Library 'alibi' is already installed.\n"
     ]
    }
   ],
   "source": [
    "# Check and install required libraries from the libraries.json file\n",
    "import json\n",
    "\n",
    "# Read the libraries from the text file\n",
    "with open('../Libraries/Part1_Lib.json', 'r') as file:\n",
    "    libraries = json.load(file)\n",
    "\n",
    "# ANSI escape codes for colored output\n",
    "GREEN = \"\\033[92m\"  # Green text\n",
    "RED = \"\\033[91m\"    # Red text\n",
    "RESET = \"\\033[0m\"   # Reset to default color\n",
    "\n",
    "# Function to check and install libraries\n",
    "def check_and_install_libraries(libraries):\n",
    "    for lib, import_name in libraries.items():\n",
    "        try:\n",
    "            # Attempt to import the library\n",
    "            __import__(import_name)\n",
    "            print(f\"[{GREEN}✔{RESET}] Library '{lib}' is already installed.\")\n",
    "        except ImportError:\n",
    "            # If import fails, try to install the library\n",
    "            print(f\"[{RED}✖{RESET}] Library '{lib}' is not installed. Installing...\")\n",
    "            %pip install {lib}\n",
    "\n",
    "# Execute the function to check and install libraries\n",
    "check_and_install_libraries(libraries)\n",
    "\n",
    "# Import necessary libraries for data analysis and modeling\n",
    "import warnings                                                                     # Disable warnings\n",
    "import pandas as pd                                                                 # Data manipulation and analysis                #type: ignore\n",
    "import numpy as np                                                                  # Numerical computations                        #type: ignore\n",
    "import matplotlib.pyplot as plt                                                     # Data visualization                            #type: ignore\n",
    "import seaborn as sns                                                               # Statistical data visualization                #type: ignore\n",
    "import statsmodels.formula.api as smf                                               # Statistical models                            #type: ignore\n",
    "# Alibi imports for the MNIST example\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(40)  # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior()  # disable TF2 behaviour as Alibi code still relies on TF1 constructs\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Flatten, MaxPooling2D, UpSampling2D, Dense, Input  \n",
    "from tensorflow.keras.models import Model, Sequential, load_model   \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split                                # Train-test split                              #type: ignore                                                              \n",
    "from tensorflow.keras.optimizers import Adam                                        # Neural network optimizer                      #type: ignore\n",
    "from sklearn.preprocessing import StandardScaler,  OneHotEncoder                    # Data scaling                                  #type: ignore\n",
    "from sklearn.impute import SimpleImputer                                            # Missing value imputation                      #type: ignore\n",
    "from sklearn.inspection import PartialDependenceDisplay, permutation_importance     # Feature importance                            #type: ignore\n",
    "from alibi.explainers import ALE, plot_ale                                          # ALE plots                                     #type: ignore\n",
    "from sklearn.neural_network import MLPClassifier                                    # Neural network classifier                     #type: ignore\n",
    "from sklearn.metrics import accuracy_score                                          # Model evaluation                              #type: ignore\n",
    "import statsmodels.api as sm \n",
    "from alibi.explainers import Counterfactual\n",
    "from time import time                                                       # Statistical models                            #type: ignore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "from alibi.explainers import CounterfactualProto\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\") \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../Datasets/Titanic/train.csv' dataset loaded successfully.\n",
      "'../Datasets/Titanic/test.csv' dataset loaded successfully.\n",
      "'../Datasets/Titanic/gender_submission.csv' dataset loaded successfully.\n",
      "\n",
      "Train Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "\n",
      "Train Dataset Statistical Summary:\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Test Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.1+ KB\n",
      "None\n",
      "\n",
      "Test Dataset Statistical Summary:\n",
      "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
      "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
      "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
      "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
      "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
      "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
      "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
      "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
      "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200\n",
      "\n",
      "Gender Submission Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   PassengerId  418 non-null    int64\n",
      " 1   Survived     418 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 6.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the filenames\n",
    "train_filename = '../Datasets/Titanic/train.csv'\n",
    "test_filename = '../Datasets/Titanic/test.csv'\n",
    "gender_submission_filename = '../Datasets/Titanic/gender_submission.csv'\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    train_data = pd.read_csv(train_filename)\n",
    "    test_data = pd.read_csv(test_filename)\n",
    "    gender_submission_data = pd.read_csv(gender_submission_filename)\n",
    "    print(f\"'{train_filename}' dataset loaded successfully.\")\n",
    "    print(f\"'{test_filename}' dataset loaded successfully.\")\n",
    "    print(f\"'{gender_submission_filename}' dataset loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e.filename} was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except pd.errors.EmptyDataError as e:\n",
    "    print(f\"Error: {e.filename} is empty.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error: There was a problem parsing {e.filename}. Please check the file format.\")\n",
    "    exit()\n",
    "\n",
    "# Dataset insights\n",
    "print(\"\\nTrain Dataset Overview:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nTrain Dataset Statistical Summary:\")\n",
    "print(train_data.describe())\n",
    "\n",
    "print(\"\\nTest Dataset Overview:\")\n",
    "print(test_data.info())\n",
    "print(\"\\nTest Dataset Statistical Summary:\")\n",
    "print(test_data.describe())\n",
    "\n",
    "print(\"\\nGender Submission Dataset Overview:\")\n",
    "print(gender_submission_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (712, 11)\n",
      "Test data shape: (179, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load the Titanic dataset\n",
    "train_data = pd.read_csv('../Datasets/Titanic/train.csv')\n",
    "\n",
    "# Preprocessing\n",
    "# Separate features and target\n",
    "y = train_data['Survived']  # Target\n",
    "X = train_data.drop(columns=['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'])  # Features\n",
    "\n",
    "# Handle categorical variables with one-hot encoding\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "categorical_encoded = one_hot_encoder.fit_transform(X[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Drop original categorical columns and append the encoded columns\n",
    "X = X.drop(columns=categorical_features)\n",
    "X = pd.concat([X.reset_index(drop=True), categorical_encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Handle missing values with mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X.columns)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      3\u001b[0m     Input(shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),  \u001b[38;5;66;03m# Define input shape explicitly\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      6\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Output layer for binary classification\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary_crossentropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training_v1.py:321\u001b[0m, in \u001b[0;36mModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_eagerly:\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession keyword arguments are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen `run_eagerly=True`. You passed the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_kwargs,)\n\u001b[0;32m    319\u001b[0m         )\n\u001b[1;32m--> 321\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m is_any_keras_optimizer_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    323\u001b[0m     (\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(opt, optimizer_v1\u001b[38;5;241m.\u001b[39mOptimizer)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[0;32m    328\u001b[0m )\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    331\u001b[0m     is_any_keras_optimizer_v1\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions()\n\u001b[0;32m    333\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training_v1.py:1473\u001b[0m, in \u001b[0;36mModel._set_optimizer\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m [optimizers\u001b[38;5;241m.\u001b[39mget(opt) \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m optimizer]\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype_policy\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_float16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, loss_scale_optimizer\u001b[38;5;241m.\u001b[39mLossScaleOptimizer\n\u001b[0;32m   1477\u001b[0m ):\n\u001b[0;32m   1478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\optimizers\\__init__.py:297\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m         optimizer_name \u001b[38;5;241m=\u001b[39m identifier\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    291\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is a known slowdown when using v2.11+ Keras optimizers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon M1/M2 Macs. Falling back to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy Keras optimizer, i.e., \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m         )\n\u001b[1;32m--> 297\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_legacy_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Wrap legacy TF optimizer instances\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mOptimizer):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\optimizers\\__init__.py:221\u001b[0m, in \u001b[0;36mconvert_to_legacy_optimizer\u001b[1;34m(optimizer)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`convert_to_legacy_optimizer` should only be called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon instances of `tf.keras.optimizers.Optimizer`, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    220\u001b[0m optimizer_name \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m--> 221\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Remove fields that only exist in experimental optimizer.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m keys_to_remove \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_ema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_legacy_optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    230\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\optimizers\\adam.py:207\u001b[0m, in \u001b[0;36mAdam.get_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    203\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_config()\n\u001b[0;32m    205\u001b[0m     config\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    206\u001b[0m         {\n\u001b[1;32m--> 207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_hyperparameter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learning_rate\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1,\n\u001b[0;32m    211\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2,\n\u001b[0;32m    212\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon,\n\u001b[0;32m    213\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamsgrad,\n\u001b[0;32m    214\u001b[0m         }\n\u001b[0;32m    215\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\optimizers\\optimizer.py:734\u001b[0m, in \u001b[0;36m_BaseOptimizer._serialize_hyperparameter\u001b[1;34m(self, hyperparameter)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m learning_rate_schedule\u001b[38;5;241m.\u001b[39mserialize(hyperparameter)\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hyperparameter, tf\u001b[38;5;241m.\u001b[39mVariable):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhyperparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hyperparameter):\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hyperparameter()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:648\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    647\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_value()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m--> 648\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# Build the feed-forward neural network\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Define input shape explicitly\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model - MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (MLPClassifier): 0.800561797752809\n"
     ]
    }
   ],
   "source": [
    "# Train a surrogate model (MLPClassifier)\n",
    "surrogate_model = MLPClassifier(hidden_layer_sizes=(32,), activation='logistic', random_state=1, max_iter=1000).fit(X_train, y_train)\n",
    "print('Accuracy (MLPClassifier): ' + str(surrogate_model.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.1\n",
    "\n",
    "### Set up Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:alibi.explainers.counterfactual:Currently only single instance explanations supported (first dim = 1), but first dim = 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrectly predicted samples: 120\n",
      "Sample index: 1, Actual label: 0, Predicted: 1\n",
      "Sample features (scaled):\n",
      "       Pclass       Age     SibSp     Parch      Fare  Sex_female  Sex_male  \\\n",
      "439 -0.369365  0.100109 -0.474545 -0.473674 -0.437007   -0.737695  0.737695   \n",
      "\n",
      "     Embarked_C  Embarked_Q  Embarked_S  Embarked_nan  \n",
      "439   -0.482043   -0.307562    0.619306     -0.047431  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 2) for Tensor cf_search/assign_target:0, which has shape (11, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 56\u001b[0m\n\u001b[0;32m     43\u001b[0m cf_explainer \u001b[38;5;241m=\u001b[39m Counterfactual(\n\u001b[0;32m     44\u001b[0m     predict_fn\u001b[38;5;241m=\u001b[39mpredict_fn,\n\u001b[0;32m     45\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],),  \u001b[38;5;66;03m# e.g. if X_train has 11 columns => shape=(11,)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     learning_rate_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 8. Generate a counterfactual explanation\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#    x_test_sample[0] gives us a rank-1 array of shape (n_features,)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mcf_explainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 9. Print results\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Counterfactual Explanation ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\alibi\\explainers\\counterfactual.py:366\u001b[0m, in \u001b[0;36mCounterfactual.explain\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    363\u001b[0m X_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(X)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# minimize loss iteratively\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_minimize_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_dict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\alibi\\explainers\\counterfactual.py:506\u001b[0m, in \u001b[0;36mCounterfactual._minimize_loss\u001b[1;34m(self, X, X_init, Y)\u001b[0m\n\u001b[0;32m    504\u001b[0m lam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size) \u001b[38;5;241m*\u001b[39m l_step\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_init)\n\u001b[1;32m--> 506\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_orig\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_cf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_current\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_target\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_ohe\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[0;32m    511\u001b[0m \n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# numerical gradients\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     grads_num \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_shape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\client\\session.py:1165\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1161\u001b[0m   np_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(subfeed_val, dtype\u001b[38;5;241m=\u001b[39msubfeed_dtype)\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_tensor_handle_feed \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m subfeed_t\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39mis_compatible_with(np_val\u001b[38;5;241m.\u001b[39mshape)):\n\u001b[1;32m-> 1165\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1166\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot feed value of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(np_val\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for Tensor \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1167\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfeed_t\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which has shape \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1168\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(subfeed_t\u001b[38;5;241m.\u001b[39mget_shape())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mis_feedable(subfeed_t):\n\u001b[0;32m   1170\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfeed_t\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m may not be fed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 2) for Tensor cf_search/assign_target:0, which has shape (11, 2)"
     ]
    }
   ],
   "source": [
    "# 1. Make predictions on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# 2. Identify misclassified samples\n",
    "incorrect_indices = np.where(y_pred != y_test.values)[0]\n",
    "print(f\"Number of incorrectly predicted samples: {len(incorrect_indices)}\")\n",
    "\n",
    "if len(incorrect_indices) < 1:\n",
    "    print(\"No misclassified samples found. Cannot generate counterfactuals.\")\n",
    "else:\n",
    "    # 3. Select one misclassified example\n",
    "    sample_idx = incorrect_indices[0]  # pick the first one for demonstration\n",
    "    # X_test.iloc[[sample_idx]] is shape (1, 11)\n",
    "    x_test_sample = X_test.iloc[[sample_idx]].values  \n",
    "    actual_label = y_test.values[sample_idx]\n",
    "    print(f\"Sample index: {sample_idx}, Actual label: {actual_label}, Predicted: {y_pred[sample_idx]}\")\n",
    "    print(\"\\nSample features (scaled):\")\n",
    "    display(X_test.iloc[[sample_idx]])\n",
    "\n",
    "    # 4. Define a new predict_fn that outputs [p(died), p(survived)]\n",
    "    def predict_fn(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        x is expected to be shape (batch_size, n_features).\n",
    "        For a single sample, shape is (1, 11).\n",
    "        \"\"\"\n",
    "        # If x is rank-1, reshape to (1, n_features).\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        # model.predict(...) returns shape (batch_size, 1) for single sigmoid output.\n",
    "        p_survived = model.predict(x).flatten()  # shape (batch_size,)\n",
    "        # Convert to shape (batch_size, 2): [p(died), p(survived)] for each sample.\n",
    "        p_died = 1.0 - p_survived\n",
    "        return np.vstack([p_died, p_survived]).T  # shape (batch_size, 2)\n",
    "\n",
    "    # 5. Determine feature_range from training data\n",
    "    lower_bounds = X_train.min(axis=0).values\n",
    "    upper_bounds = X_train.max(axis=0).values\n",
    "    feature_range = (lower_bounds, upper_bounds)\n",
    "\n",
    "    # 6. Decide on target_proba to 'flip' the original label\n",
    "    #    If actual_label=0 (died), aim for Survived -> set target_proba > 0.5 (e.g. 0.8)\n",
    "    #    If actual_label=1 (survived), aim for Died -> set target_proba < 0.5 (e.g. 0.2)\n",
    "    desired_proba = 0.8 if actual_label == 0 else 0.2\n",
    "\n",
    "    # 7. Instantiate the Counterfactual explainer with shape=(1, 11)\n",
    "    #    This ensures Alibi interprets your data as 1 sample, 11 features, 2 classes.\n",
    "    cf_explainer = Counterfactual(\n",
    "        predict_fn=predict_fn,\n",
    "        shape=(1, X_train.shape[1]),  # e.g. shape=(1, 11)\n",
    "        target_proba=desired_proba,\n",
    "        max_iter=1000,\n",
    "        feature_range=feature_range,\n",
    "        lam_init=1e-1,\n",
    "        max_lam_steps=10,\n",
    "        learning_rate_init=1e-2\n",
    "    )\n",
    "\n",
    "    # 8. Generate a counterfactual explanation\n",
    "    #    x_test_sample is shape (1, 11), so no [0] indexing needed!\n",
    "    explanation = cf_explainer.explain(x_test_sample)\n",
    "\n",
    "    # 9. Print results\n",
    "    print(\"\\n--- Counterfactual Explanation ---\")\n",
    "    print(\"Original 2-column probability:\", predict_fn(x_test_sample))\n",
    "    if explanation.cf is not None:\n",
    "        cf_sample = explanation.cf['X']  # shape is (1, 11)\n",
    "        print(\"\\nCounterfactual feature values (scaled):\")\n",
    "        display(cf_sample)\n",
    "\n",
    "        print(\"Counterfactual 2-column probability:\", predict_fn(cf_sample))\n",
    "        \n",
    "        # Show the numerical difference between original and CF\n",
    "        changes = cf_sample[0] - x_test_sample[0]\n",
    "        print(\"\\nDifference between CF and original sample:\")\n",
    "        for col, diff in zip(X_test.columns, changes):\n",
    "            print(f\"{col}: {diff:.3f}\")\n",
    "    else:\n",
    "        print(\"No counterfactual found within the specified parameters.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
